{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## USE CLS TOKEN"
      ],
      "metadata": {
        "id": "ENxffghEQyW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0HrOpukpm2I",
        "outputId": "cbfaa565-8487-4806-a75a-78ab8e72a999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "from timm.data import resolve_data_config\n",
        "from timm.data.transforms_factory import create_transform\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vit = timm.create_model('vit_base_patch16_224', pretrained=True).eval()\n",
        "config = resolve_data_config({}, model=vit)\n",
        "transform = create_transform(**config)\n",
        "\n",
        "def extract_original_filename(path):\n",
        "    \"\"\"Extract the original ILSVRC filename from adversarial image paths\"\"\"\n",
        "    basename = os.path.basename(path)\n",
        "    match = re.search(r'ILSVRC2012_val_\\d+\\.JPEG', basename)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    return basename\n",
        "\n",
        "def save_used_basenames(train_csv, val_csv, output_path):\n",
        "    used = set()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    train_used = set(train_df['image_path'].apply(extract_original_filename))\n",
        "    used.update(train_used)\n",
        "    print(f\"Found {len(train_used)} unique images in train adversarial dataset\")\n",
        "\n",
        "    val_df = pd.read_csv(val_csv)\n",
        "    val_used = set(val_df['image_path'].apply(extract_original_filename))\n",
        "    used.update(val_used)\n",
        "    print(f\"Found {len(val_used)} unique images in val adversarial dataset\")\n",
        "\n",
        "    print(f\"Total unique images across both datasets: {len(used)}\")\n",
        "    print(f\"Overlap between train and val adversarial: {len(train_used.intersection(val_used))}\")\n",
        "\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(used, f)\n",
        "    print(f\"Saved {len(used)} original ILSVRC basenames to {output_path}\")\n",
        "    print(\"These images will be excluded when adding extra clean samples to training\")\n",
        "\n",
        "def load_used_basenames(pkl_path):\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        used = pickle.load(f)\n",
        "    return used\n",
        "\n",
        "used_filenames = load_used_basenames('/content/drive/MyDrive/my231n/used_basenames.pkl')\n",
        "\n",
        "def get_extra_clean_examples(val_dir, used_filenames, n=7000):\n",
        "    \"\"\"Get extra clean examples from val directory, excluding already used images\"\"\"\n",
        "    all_clean = []\n",
        "    for root, _, files in os.walk(val_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.JPEG') and file not in used_filenames:\n",
        "                all_clean.append(os.path.relpath(os.path.join(root, file), val_dir))\n",
        "\n",
        "    if len(all_clean) < n:\n",
        "        print(f\"Warning: Requested {n} clean samples, but only found {len(all_clean)} unused ones.\")\n",
        "        print(f\"Using all {len(all_clean)} available unused samples.\")\n",
        "        n = len(all_clean)\n",
        "\n",
        "    sampled = random.sample(all_clean, n)\n",
        "    new_rows = [{\n",
        "        'image_path': os.path.join('val', path),\n",
        "        'attack_type': 'clean',\n",
        "        'original_class': -1\n",
        "    } for path in sampled]\n",
        "    return pd.DataFrame(new_rows)\n",
        "\n",
        "class AdversarialDetectionDataset(Dataset):\n",
        "    def __init__(self, metadata_csv, root_dir, split, transform):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        if split == 'train':\n",
        "            self.df = pd.read_csv(metadata_csv)\n",
        "            self.df = self.df[self.df['attack_type'] != 'CW']\n",
        "\n",
        "            val_df = pd.read_csv('/content/drive/MyDrive/my231n/adversarial_val_dataset/metadata_with_clean.csv')\n",
        "            val_original_names = set(val_df['image_path'].apply(extract_original_filename))\n",
        "\n",
        "            original_len = len(self.df)\n",
        "            train_original_names = self.df['image_path'].apply(extract_original_filename)\n",
        "            overlap_mask = train_original_names.isin(val_original_names)\n",
        "            self.df = self.df[~overlap_mask]\n",
        "\n",
        "            print(f\"Removed {overlap_mask.sum()} overlapping images from training set\")\n",
        "            print(f\"Training set reduced from {original_len} to {len(self.df)} samples\")\n",
        "\n",
        "            clean_df = self.df[self.df['attack_type'].str.lower() == 'clean']\n",
        "            adv_df = self.df[self.df['attack_type'].str.lower() != 'clean']\n",
        "\n",
        "            extra_clean_df = get_extra_clean_examples(\n",
        "                val_dir=os.path.join(root_dir, 'val'),\n",
        "                used_filenames=used_filenames,\n",
        "                n=8512\n",
        "            )\n",
        "            print(f\"Dataset composition: {len(clean_df)} original clean, {len(adv_df)} adversarial, {len(extra_clean_df)} extra clean\")\n",
        "            self.df = pd.concat([clean_df, adv_df, extra_clean_df], ignore_index=True)\n",
        "\n",
        "        elif split == 'val':\n",
        "            self.df = pd.read_csv(metadata_csv)\n",
        "            split_keyword = 'adversarial_val_dataset'\n",
        "            self.df = self.df[self.df['image_path'].str.contains(split_keyword)]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported split: {split}\")\n",
        "\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        if len(self.df) == 0:\n",
        "            raise ValueError(f\"No data found for split '{split}'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        rel_path = row['image_path']\n",
        "        full_path = rel_path if os.path.isabs(rel_path) else os.path.join(self.root_dir, rel_path)\n",
        "        label = 1 if row['attack_type'].lower() == 'clean' else 0\n",
        "\n",
        "        try:\n",
        "            image = Image.open(full_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {full_path}: {e}\")\n",
        "            image = torch.zeros((3, 224, 224))\n",
        "            label = -1\n",
        "\n",
        "        return image, label\n",
        "\n",
        "metadata_csv_path_train = '/content/drive/MyDrive/my231n/adversarial_train_dataset/metadata_with_clean.csv'\n",
        "metadata_csv_path_val = '/content/drive/MyDrive/my231n/adversarial_val_dataset/metadata_with_clean.csv'\n",
        "image_root = '/content/drive/MyDrive/my231n/'\n",
        "\n",
        "train_dataset = AdversarialDetectionDataset(metadata_csv_path_train, image_root, split='train', transform=transform)\n",
        "val_dataset = AdversarialDetectionDataset(metadata_csv_path_val, image_root, split='val', transform=transform)\n",
        "\n",
        "from collections import Counter\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"Loaded {len(train_dataset)} training and {len(val_dataset)} validation samples.\")\n",
        "\n",
        "def verify_no_overlap():\n",
        "    train_original_names = set()\n",
        "    val_original_names = set()\n",
        "\n",
        "    for _, row in train_dataset.df.iterrows():\n",
        "        original_name = extract_original_filename(row['image_path'])\n",
        "        train_original_names.add(original_name)\n",
        "\n",
        "    for _, row in val_dataset.df.iterrows():\n",
        "        original_name = extract_original_filename(row['image_path'])\n",
        "        val_original_names.add(original_name)\n",
        "\n",
        "    overlap = train_original_names.intersection(val_original_names)\n",
        "    print(f\"Overlap verification: {len(overlap)} overlapping images found\")\n",
        "    if len(overlap) > 0:\n",
        "        print(f\"Warning: Found overlapping images: {list(overlap)[:5]}...\")\n",
        "    else:\n",
        "        print(\"✓ No overlap detected between train and validation sets\")\n",
        "verify_no_overlap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbG6GcLeTQHd",
        "outputId": "df1f72af-c0b7-46f1-829e-1e286e24ff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Removed 2000 overlapping images from training set\n",
            "Training set reduced from 9728 to 7728 samples\n",
            "Dataset composition: 966 original clean, 6762 adversarial, 8512 extra clean\n",
            "Loaded 16240 training and 13068 validation samples.\n",
            "Overlap verification: 0 overlapping images found\n",
            "✓ No overlap detected between train and validation sets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiubJXcHP3wI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import timm\n",
        "\n",
        "class Detector(nn.Module):\n",
        "    def __init__(self,vit, num_blocks=3):\n",
        "        super().__init__()\n",
        "        self.vit = vit\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(vit.num_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self.num_blocks = num_blocks\n",
        "    def forward(self, x):\n",
        "        x = self.vit.patch_embed(x)\n",
        "        cls_token = self.vit.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = x + self.vit.pos_embed\n",
        "        x = self.vit.pos_drop(x)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.vit.blocks[i](x)\n",
        "\n",
        "        x = self.vit.norm(x)\n",
        "        feats = x[:, 0]\n",
        "\n",
        "        out = self.fc(feats)\n",
        "        return torch.sigmoid(out).squeeze()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "vit.eval()\n",
        "device='cuda'\n",
        "detector = Detector(vit,6).to(device)\n",
        "\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "\n",
        "optimizer = Adam(detector.parameters(), lr=1e-4)\n",
        "print(len(train_loader))\n",
        "\n",
        "for epoch in range(10):\n",
        "    detector.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device).float()\n",
        "        preds = detector(images)\n",
        "        loss = F.binary_cross_entropy(preds, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "        predicted_labels = (preds >= 0.5).float()\n",
        "        total_correct += (predicted_labels == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "    torch.save(detector.state_dict(), f'detector_epoch_{epoch+1}.pth')\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4fdYjCaRerh",
        "outputId": "478faed4-3c8b-4d3c-ad51-575ef7fd7735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 508/508 [24:44<00:00,  2.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Avg Loss: 0.2277, Accuracy: 0.9181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 508/508 [01:01<00:00,  8.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Avg Loss: 0.1974, Accuracy: 0.9315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 508/508 [01:01<00:00,  8.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Avg Loss: 0.1903, Accuracy: 0.9342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 508/508 [01:01<00:00,  8.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Avg Loss: 0.1912, Accuracy: 0.9344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 508/508 [01:01<00:00,  8.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Avg Loss: 0.1949, Accuracy: 0.9323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 508/508 [01:01<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Avg Loss: 0.1926, Accuracy: 0.9337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 508/508 [01:01<00:00,  8.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Avg Loss: 0.1832, Accuracy: 0.9381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 508/508 [01:01<00:00,  8.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Avg Loss: 0.1866, Accuracy: 0.9363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 508/508 [01:01<00:00,  8.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Avg Loss: 0.1894, Accuracy: 0.9349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 508/508 [01:01<00:00,  8.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Avg Loss: 0.1893, Accuracy: 0.9355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(detector.state_dict(), f'detector_epoch_final.pth')"
      ],
      "metadata": {
        "id": "UN0U-nmxJwc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "detector.eval()\n",
        "val_loss = 0\n",
        "val_correct = 0\n",
        "val_total = 0\n",
        "from itertools import islice\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device).float()\n",
        "        preds = detector(images)\n",
        "        loss = F.binary_cross_entropy(preds, labels)\n",
        "\n",
        "        val_loss += loss.item() * images.size(0)\n",
        "        predicted_labels = (preds >= 0.5).float()\n",
        "        val_correct += (predicted_labels == labels).sum().item()\n",
        "        val_total += labels.size(0)\n",
        "\n",
        "val_avg_loss = val_loss / val_total\n",
        "val_accuracy = val_correct / val_total\n",
        "\n",
        "print(f\"[Final Validation] Loss: {val_avg_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "uuR5IE8EfbPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44696da9-d519-4fb2-dd29-d19552006434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Final Validation] Loss: 0.4176, Accuracy: 0.8783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USE POOLING"
      ],
      "metadata": {
        "id": "vHTy-PWSdGv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import timm\n",
        "\n",
        "class Detector(nn.Module):\n",
        "    def __init__(self,vit, num_blocks=3):\n",
        "        super().__init__()\n",
        "        self.vit = vit\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(vit.num_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self.num_blocks = num_blocks\n",
        "    def forward(self, x):\n",
        "        x = self.vit.patch_embed(x)\n",
        "        cls_token = self.vit.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x = x + self.vit.pos_embed\n",
        "        x = self.vit.pos_drop(x)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            x = self.vit.blocks[i](x)\n",
        "\n",
        "        x = self.vit.norm(x)\n",
        "        feats = x[:, 1:, :].mean(dim=1)\n",
        "\n",
        "        out = self.fc(feats)\n",
        "        return torch.sigmoid(out).squeeze()\n",
        "\n",
        "\n",
        "vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "vit.eval()\n",
        "device='cuda'\n",
        "detector = Detector(vit).to(device)\n"
      ],
      "metadata": {
        "id": "hngKp94ydMbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "\n",
        "optimizer = Adam(detector.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(10):\n",
        "    detector.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for i, (images, labels, _, _, _) in enumerate(train_loader, 1):\n",
        "        print(f\"Epoch {epoch}, Step {i}\")\n",
        "        images, labels = images.to(device), labels.to(device).float()\n",
        "        preds = detector(images)\n",
        "        loss = F.binary_cross_entropy(preds, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        predicted_labels = (preds >= 0.5).float()\n",
        "        total_correct += (predicted_labels == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    train_avg_loss = total_loss / total_samples\n",
        "    train_accuracy = total_correct / total_samples\n",
        "    print(f\"[Train] Epoch {epoch+1}, Loss: {train_avg_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    detector.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, _, _, _ in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device).float()\n",
        "            preds = detector(images)\n",
        "            loss = F.binary_cross_entropy(preds, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            predicted_labels = (preds >= 0.5).float()\n",
        "            val_correct += (predicted_labels == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_avg_loss = val_loss / val_total\n",
        "    val_accuracy = val_correct / val_total\n",
        "    print(f\"[Val]   Epoch {epoch+1}, Loss: {val_avg_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC5RmqKrdb6X",
        "outputId": "ef87a4cd-874b-4c26-9b14-5e19a02c0616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "304\n",
            "Epoch 1, Avg Loss: 0.3847, Accuracy: 0.8750\n",
            "Epoch 2, Avg Loss: 0.3810, Accuracy: 0.8750\n",
            "Epoch 3, Avg Loss: 0.3799, Accuracy: 0.8750\n",
            "Epoch 4, Avg Loss: 0.3822, Accuracy: 0.8750\n",
            "Epoch 5, Avg Loss: 0.3794, Accuracy: 0.8750\n",
            "Epoch 6, Avg Loss: 0.3794, Accuracy: 0.8750\n",
            "Epoch 7, Avg Loss: 0.3783, Accuracy: 0.8750\n",
            "Epoch 8, Avg Loss: 0.3789, Accuracy: 0.8750\n",
            "Epoch 9, Avg Loss: 0.3786, Accuracy: 0.8750\n",
            "Epoch 10, Avg Loss: 0.3781, Accuracy: 0.8750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save"
      ],
      "metadata": {
        "id": "cPNwviA_JVNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}